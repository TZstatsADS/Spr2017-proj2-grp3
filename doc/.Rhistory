dtm.idf
ap_lda <- LDA(dtm, k = 4)
ap_lda
ap_lda_td <- tidy(ap_lda)
ap_lda_td
View(ap_lda_td)
ap_top_terms <- ap_lda_td %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_bar(stat = "identity", show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
selectInput("input","", president.list)
install.packages(selectize.js)
install.packages("selectize.js")
install.packages("shiny")
selectInput("input","", president.list)
library(shiny)
selectInput("input","", president.list)
input
selectInput("data","", president.list)
get(input$data)
get(data)
library(shiny)
ui <- fluidPage(selectInput("data","", president.list),
plotOutput(outputId = "hist")
)
server <- function(input, output) {
output$hist <- renderPlot({
hist(rnorm(input$data))
})
}
shinyApp(ui = ui, server = server)
library(shiny)
ui <- fluidPage(selectInput("data","", president.list),
plotOutput(outputId = "hist")
)
server <- function(input, output) {
selectedData <- reactive({
term1 <- paste0(input$data,"-1")
term2 <- paste0(input$data,"-2")
selected.data1 <- dtm.idf.tidy[dtm.idf.tidy$document==term1,]
})
output$wordclouds <- renderPlot({
wordcloud(selected.data1$word, selected.data1$count,
main = "1st Term",
scale=c(5,0.5),
max.words=70,
min.freq=1,
random.order=FALSE,
rot.per=0.0,
use.r.layout=T,
random.color=FALSE,
colors=brewer.pal(9,"Blues"))
})
}
shinyApp(ui = ui, server = server)
library(shiny)
ui <- fluidPage(selectInput("data","", president.list),
plotOutput('worclouds')
)
server <- function(input, output) {
selectedData <- reactive({
term1 <- paste0(input$data,"-1")
term2 <- paste0(input$data,"-2")
selected.data1 <- dtm.idf.tidy[dtm.idf.tidy$document==term1,]
})
output$wordclouds <- renderPlot({
wordcloud(selected.data1$word, selected.data1$count,
main = "1st Term",
scale=c(5,0.5),
max.words=70,
min.freq=1,
random.order=FALSE,
rot.per=0.0,
use.r.layout=T,
random.color=FALSE,
colors=brewer.pal(9,"Blues"))
})
}
shinyApp(ui = ui, server = server)
packages.used=c("tidytext", "dplyr", "tm", "ggplot2", "wordcloud", "rvest",
"tibble", "qdap", "sentimentr", "gplots", "syuzhet",
"factoextra", "beeswarm", "scales", "RColorBrewer", "RANN",
"topicmodels", "splitstackshape", "tidyr")
# check packages that need to be installed.
packages.needed=setdiff(packages.used, intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE,
repos='http://cran.us.r-project.org')
}
# load packages
library("tidytext")
library("dplyr")
library("tm")
library("ggplot2")
library("wordcloud")
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("topicmodels")
library("splitstackshape")
library("tidyr")
# source functions
source("../lib/processing.R")
source("../lib/displaying.R")
print(R.version)
folder.path = "../data/InauguralSpeeches/"
speeches = list.files(path = folder.path, pattern = "*.txt")
ff.all <- Corpus(DirSource(folder.path))
# remove white Spaces, transform to lower case, remove stop words, remove empty words and remove punctuation
ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removeWords, character(0))
ff.all<-tm_map(ff.all, removePunctuation)
# compute Term-Document Matrix
tdm <- TermDocumentMatrix(ff.all)
# compute Document-Term Matrix
dtm <- DocumentTermMatrix(ff.all)
dtm.tidy <- tidy(dtm)
dtm.all <- summarise(group_by(dtm.tidy, term), sum(count))
# # inspect dtm or tdm
# inspect(tdm[100:105,25:30])
# inspect(dtm[25:27,100:104])
# dim(dtm.idf)
#Note: I am using both tdm and dtm, because it's easier to use one or the other for some applications later on.
# Compute TermDocument Matrix weighted with TF-IDF principle
tdm.idf <- TermDocumentMatrix(ff.all,
control = list(weighting = function(x)
weightTfIdf(x, normalize =FALSE),
stopwords = TRUE))
# Compute DocumentTerm Matrix weighted with TF-IDF principle
dtm.idf <- DocumentTermMatrix(ff.all,
control = list(weighting = function(x)
weightTfIdf(x, normalize =FALSE),
stopwords = TRUE))
dtm.idf.tidy <- tidy(dtm.idf)
dtm.idf.all <- summarise(group_by(dtm.idf.tidy, term), sum(count))
# # Cast into a Matrix object
# m.tdm.idf.tidy <- tdm.idf.tidy %>%
#   cast_sparse(document, term, count)
# list of presidents who did two terms in office
prex.out <- substr(speeches, 6, nchar(speeches)-4)
speeches.info <- as.data.frame(prex.out)
speeches.info <- cSplit(speeches.info, "prex.out", "-")
colnames(speeches.info)[1] <- "name"
colnames(speeches.info)[2] <- "term"
speeches.info$prex.out_3 <- NULL
president.list <- as.character(speeches.info[speeches.info$term==2,]$name)
print(president.list)
# update all dtms tables so that they contain only speeches given by presidents who stayed two terms in office
# see the file processing.R for details about the functions used in this chunk
dtm.tidy <- update.tables(dtm.tidy,president.list)
dtm.idf.tidy <- update.tables(dtm.idf.tidy,president.list)
dtm.all <- summarise(group_by(dtm.tidy, word), count = sum(count))
dtm.idf.all <- summarise(group_by(dtm.idf.tidy, word), count = sum(count))
library(shiny)
ui <- fluidPage(selectInput(inputId = "data","", president.list),
plotOutput(outpuId = "worclouds")
)
ui <- fluidPage(selectInput(inputId = "data","", president.list),
plotOutput(outputId = "worclouds")
)
server <- function(input, output) {
selectedData <- reactive({
term1 <- paste0(input$data,"-1")
selected.data1 <- dtm.idf.tidy[dtm.idf.tidy$document==term1,]
})
output$wordclouds <- renderPlot({
wordcloud(selected.data1$word, selected.data1$count,
main = "1st Term",
scale=c(5,0.5),
max.words=70,
min.freq=1,
random.order=FALSE,
rot.per=0.0,
use.r.layout=T,
random.color=FALSE,
colors=brewer.pal(9,"Blues"))
})
}
shinyApp(ui = ui, server = server)
ui <- fluidPage(selectInput(inputId = "data","", president.list),
plotOutput(outputId = "worclouds")
)
server <- function(input, output) {
selectedData <- reactive({
term1 <- paste0(input$data,"-1")
selected.data1 <- dtm.idf.tidy[dtm.idf.tidy$document==term1,]
})
output$wordclouds <- renderPlot({
wordcloud(selected.data1$word, selected.data1$count,
main = "1st Term",
scale=c(5,0.5),
max.words=70,
min.freq=1,
random.order=FALSE,
rot.per=0.0,
use.r.layout=T,
random.color=FALSE,
colors=brewer.pal(9,"Blues"))
})
}
shinyApp(ui = ui, server = server)
ui <- fluidPage(selectInput(inputId = "data","", president.list),
plotOutput(outputId = "worclouds")
)
server <- function(input, output) {
selectedData <- reactive({
term1 <- paste0(input$data,"-1")
selected.data1 <- dtm.idf.tidy[dtm.idf.tidy$document==term1,]
})
output$wordclouds <- renderPlot({
print("a")
})
}
shinyApp(ui = ui, server = server)
ui <- fluidPage(selectInput(inputId = "data","", president.list),
plotOutput(outputId = "worcdlouds")
)
server <- function(input, output) {
selectedData <- reactive({
term1 <- paste0(input$data,"-1")
selected.data1 <- dtm.idf.tidy[dtm.idf.tidy$document==term1,]
})
output$wordclouds <- renderPlot({
print("a")
})
}
shinyApp(ui = ui, server = server)
ui <- fluidPage(selectInput(inputId = "data","", president.list),
plotOutput(outputId = "wordclouds")
)
packages.used=c("tidytext", "dplyr", "tm", "ggplot2", "wordcloud", "rvest",
"tibble", "qdap", "sentimentr", "gplots", "syuzhet",
"factoextra", "beeswarm", "scales", "RColorBrewer", "RANN",
"topicmodels", "splitstackshape", "tidyr", "shiny")
# check packages that need to be installed.
packages.needed=setdiff(packages.used, intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE,
repos='http://cran.us.r-project.org')
}
# load packages
library("tidytext")
library("dplyr")
library("tm")
library("ggplot2")
library("wordcloud")
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("topicmodels")
library("splitstackshape")
library("tidyr")
library("shiny")
# source functions
source("../lib/processing.R")
source("../lib/displaying.R")
print(R.version)
folder.path = "../data/InauguralSpeeches/"
speeches = list.files(path = folder.path, pattern = "*.txt")
ff.all <- Corpus(DirSource(folder.path))
# remove white Spaces, transform to lower case, remove stop words, remove empty words and remove punctuation
ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removeWords, character(0))
ff.all<-tm_map(ff.all, removePunctuation)
# compute Term-Document Matrix
tdm <- TermDocumentMatrix(ff.all)
# compute Document-Term Matrix
dtm <- DocumentTermMatrix(ff.all)
dtm.tidy <- tidy(dtm)
dtm.all <- summarise(group_by(dtm.tidy, term), sum(count))
# # inspect dtm or tdm
# inspect(tdm[100:105,25:30])
# inspect(dtm[25:27,100:104])
# dim(dtm.idf)
#Note: I am using both tdm and dtm, because it's easier to use one or the other for some applications later on.
# Compute TermDocument Matrix weighted with TF-IDF principle
tdm.idf <- TermDocumentMatrix(ff.all,
control = list(weighting = function(x)
weightTfIdf(x, normalize =FALSE),
stopwords = TRUE))
# Compute DocumentTerm Matrix weighted with TF-IDF principle
dtm.idf <- DocumentTermMatrix(ff.all,
control = list(weighting = function(x)
weightTfIdf(x, normalize =FALSE),
stopwords = TRUE))
dtm.idf.tidy <- tidy(dtm.idf)
dtm.idf.all <- summarise(group_by(dtm.idf.tidy, term), sum(count))
# # Cast into a Matrix object
# m.tdm.idf.tidy <- tdm.idf.tidy %>%
#   cast_sparse(document, term, count)
# list of presidents who did two terms in office
prex.out <- substr(speeches, 6, nchar(speeches)-4)
speeches.info <- as.data.frame(prex.out)
speeches.info <- cSplit(speeches.info, "prex.out", "-")
colnames(speeches.info)[1] <- "name"
colnames(speeches.info)[2] <- "term"
speeches.info$prex.out_3 <- NULL
president.list <- as.character(speeches.info[speeches.info$term==2,]$name)
print(president.list)
# update all dtms tables so that they contain only speeches given by presidents who stayed two terms in office
# see the file processing.R for details about the functions used in this chunk
dtm.tidy <- update.tables(dtm.tidy,president.list)
dtm.idf.tidy <- update.tables(dtm.idf.tidy,president.list)
dtm.all <- summarise(group_by(dtm.tidy, word), count = sum(count))
dtm.idf.all <- summarise(group_by(dtm.idf.tidy, word), count = sum(count))
ui <- fluidPage(selectInput(inputId = "data","", president.list),
plotOutput(outputId = "wordclouds")
)
server <- function(input, output) {
data <- reactive({
dtm.idf.tidy[dtm.idf.tidy$document==term1,]
})
output$wordclouds <- renderPlot({
wordcloud(data()$word, data()$count,
main = "1st Term",
scale=c(5,0.5),
max.words=70,
min.freq=1,
random.order=FALSE,
rot.per=0.0,
use.r.layout=T,
random.color=FALSE,
colors=brewer.pal(9,"Blues"))
})
}
shinyApp(ui = ui, server = server)
library(ISLR)
names(Smarket)   # Contains % returns for S&P 500 for 1250 days from 2001 to 2005
dim(Smarket)     # It also contains returns for 5 previous days and previous day's trading volume
summary(Smarket)
pairs(Smarket)
pairs(Smarket)
cor(Smarket[,-9])
attach(Smarket)
names(Smarket)
plot(Volume)
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Smarket,family=binomial)
summary(glm.fit)
coef(glm.fit)
summary(glm.fit)$coef
summary(glm.fit)$coef[,4]
glm.probs=predict(glm.fit,type="response")
dim(Smarket)
glm.probs[1:10]
contrasts(Direction)
glm.pred=rep("Down",1250)  # A 1250 x 1 vector with "Down" in every position
glm.pred[glm.probs>.5]="Up"
table(glm.pred,Direction)
(507+145)/1250
mean(glm.pred==Direction)
train=(Year<2005)   # Creating separate training and test sets
Smarket.2005=Smarket[!train,]
dim(Smarket.2005)
Direction.2005=Direction[!train]
Direction.2005
predict(glm.fit,newdata=data.frame(Lag1=c(1.2,1.5),Lag2=c(1.1,-0.8)),type="response")
train=(Year<2005)   # Creating separate training and test sets
Smarket.2005=Smarket[!train,]
dim(Smarket.2005)
Direction.2005=Direction[!train]
# Notice use of subset=train argument on next line
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Smarket,family=binomial,subset=train)
glm.probs=predict(glm.fit,Smarket.2005,type="response")
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]="Up"
table(glm.pred,Direction.2005) # confusion matrix for test set
mean(glm.pred==Direction.2005)
mean(glm.pred!=Direction.2005)  # test error rate is 52% Not good at all ...
# Recall p-values are all pretty large so perhaps a good idea to throw most of them out
glm.fit=glm(Direction~Lag1+Lag2,data=Smarket,family=binomial,subset=train)  # Just using lag1 and lag2
glm.probs=predict(glm.fit,Smarket.2005,type="response")
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]="Up"
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)   # results are "better": 56% of moves correctly predicted
# But it turns out market went up 56% of the time in 2005 so results not very good really
106/(106+76)  # =58.2% = % of correct predictions when prediction was "Up"
# Can get specific predicted probabilities by passing values of explanatory variables to the fitted model
predict(glm.fit,newdata=data.frame(Lag1=c(1.2,1.5),Lag2=c(1.1,-0.8)),type="response")
library(MASS)
lda.fit=lda(Direction~Lag1+Lag2,data=Smarket,subset=train)  # fit LDA on same data
lda.fit
plot(lda.fit)
plot(lda.fit)
lda.pred=predict(lda.fit, Smarket.2005)
names(lda.pred)
lda.class=lda.pred$class
table(lda.class,Direction.2005)
sum(lda.pred$posterior[,1]>=.5)  # posterior is an n x K=2  matrix of poserior probs (n = # of data points)
sum(lda.pred$posterior[,1]<.5)
lda.pred$posterior[1:20,1]
lda.class[1:20]
sum(lda.pred$posterior[,1]>.9)
library(class)
train.X=cbind(Lag1,Lag2)[train,]
View(train.X)
if (!require("DT")) install.packages('DT')
if (!require("dtplyr")) install.packages('dtplyr')
if(!require("lubridate")) install.packages('lubridate')
library(dtplyr)
library(dplyr)
library(DT)
library(lubridate)
install.packages("shiny")
library(shiny)
runExample("01_hello")
mh2009=read.csv(file="../data/ManhattanHousing.csv")
datatable(sample_n(mh2009, 50))
mh2009=
mh2009%>%
filter(ZIP.CODE>0)%>%
mutate(region=as.character(ZIP.CODE))
count.df=mh2009%>%
group_by(region)%>%
summarise(
value=n()
)
save(count.df, file="../output/count.RData")
library(choroplethrZip)
zip_choropleth(count.df,
title       = "2009 Manhattan housing sales",
legend      = "Number of sales",
county_zoom = 36061)
library(choroplethrZip)
zip_choropleth(count.df,
title       = "2009 Manhattan housing sales",
legend      = "Number of sales",
county_zoom = 36061)
library(ggmap)
library(dplyr)
mh2009.selgeo=
mh2009%>%
sample_n(10)%>%
select(starts_with("ADD"))%>%
mutate(ADDRESS_Ext=paste(ADDRESS, "New York, NY", sep=","))%>%
mutate_geocode(ADDRESS_Ext)
library(ggmap)
ggmap(get_map("New York, New York",zoom=11,color = "bw")) +
geom_point(data=mh2009.selgeo, aes(x=lon,y=lat),  color='red')
install.packages(ggproto)
install.packages("ggproto")
install.packages("ggprotto")
library(ggmap)
ggmap(get_map("New York, New York",zoom=11,color = "bw")) +
geom_point(data=mh2009.selgeo, aes(x=lon,y=lat),  color='red')
mh2009.use=
mh2009%>%
mutate(sale.month=month(as.Date(SALE.DATE, "%m/%d/%y")))%>%
mutate(sale.price=ifelse(SALE.PRICE==0, NA, SALE.PRICE))%>%
mutate(footage=ifelse(GROSS.SQUARE.FEET==0, NA, GROSS.SQUARE.FEET))%>%
mutate(unit.price=sale.price/footage)%>%
mutate(bldg.type=substr(BUILDING.CLASS.CATEGORY, 1, 2))%>%
filter(bldg.type %in% c("10", "13", "25", "28"))%>%
arrange(bldg.type)
save(mh2009.use, file="../output/mh2009use.RData")
man.nbhd=c("Central Harlem", "Chelsea and Clinton",
"East Harlem", "Gramercy Park and Murray Hill",
"Greenwich Village and Soho", "Lower Manhattan",
"Lower East Side", "Upper East Side", "Upper West Side",
"Inwood and Washington Heights")
zip.nbhd=list(1:length(man.nbhd))
zip.nbhd[[1]]=c(10026, 10027, 10030, 10037, 10039)
zip.nbhd[[2]]=c(10001, 10011, 10018, 10019, 10020)
zip.nbhd[[3]]=c(10036, 10029, 10035)
zip.nbhd[[4]]=c(10010, 10016, 10017, 10022)
zip.nbhd[[5]]=c(10012, 10013, 10014)
zip.nbhd[[6]]=c(10004, 10005, 10006, 10007, 10038, 10280)
zip.nbhd[[7]]=c(10002, 10003, 10009)
zip.nbhd[[8]]=c(10021, 10028, 10044, 10065, 10075, 10128)
zip.nbhd[[9]]=c(10023, 10024, 10025)
zip.nbhd[[10]]=c(10031, 10032, 10033, 10034, 10040)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
runApp('~/Documents/Columbia/Applied Data Science/Project 2/App-1')
runApp('~/Documents/Columbia/Applied Data Science/Project 2/App-1')
runApp('~/Documents/Columbia/Applied Data Science/Project 2/App-1')
runApp('~/Documents/Columbia/Applied Data Science/Project 2/App-1')
runApp('~/Documents/Columbia/Applied Data Science/Project 2/App-1')
runApp('~/Documents/Columbia/Applied Data Science/Project 2/App-1')
runApp('~/Documents/Columbia/Applied Data Science/Project 2/App-1')
runApp('~/Documents/Columbia/Applied Data Science/Project 2/App-1')
